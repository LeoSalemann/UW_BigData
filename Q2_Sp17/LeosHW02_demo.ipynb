{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from pyspark.streaming import StreamingContext ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>23</td><td>application_1491969884398_0029</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-uwspar.chpwi2pvew2uzb2b2u3n1kcxqf.yx.internal.cloudapp.net:8088/proxy/application_1491969884398_0029/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.8:30060/node/containerlogs/container_1491969884398_0029_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 2, "cell_type": "code", "source": "# Create a StreamingContext and a batch interval of 1 second \nssc = StreamingContext(sc, 1) \nssc.checkpoint(\"wasb:///leo/chkpnt\") ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 3, "cell_type": "code", "source": "# Define a text file stream for the /stream folder  \nstreamRdd = ssc.textFileStream(\"wasb:///leo/stream\") ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 4, "cell_type": "code", "source": "# count the words Window length is 60, siding interval is 10. \nwords = streamRdd.flatMap(lambda line: line.split(\" \")) \npairs = words.map(lambda word: (word, 1)) \n\n# Use a sliding window of length 60 and interval 10\n# lambda a, b does the routine word counting.\n# lambda x, y is a way to detect and subtract out dupes.\nwordCounts = pairs.reduceByKeyAndWindow(lambda a, b: a + b, lambda x, y: x - y, 60, 10) \n\n# Print the first 20 elements in the DStream  \nwordCounts.pprint(num=20) ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 5, "cell_type": "code", "source": "# Run until this cell  \nssc.start() ", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now go to ssh, and do hadoop put commands like this:\n\nsshuser@hn0-uwspar:~/leo$ hadoop fs -put text1 /leo/stream/text1_1\n\nsshuser@hn0-uwspar:~/leo$ hadoop fs -put text1 /leo/stream/text1_2\n\nsshuser@hn0-uwspar:~/leo$ hadoop fs -put text1 /leo/stream/text2_1\n\nsshuser@hn0-uwspar:~/leo$ hadoop fs -put text1 /leo/stream/text2_2", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "# Stop the stream to see the results.  Toggle beteen stop/start to do more.\nssc.stop()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-------------------------------------------\nTime: 2017-04-19 18:33:39\n-------------------------------------------\n('stood', 4)\n('scout', 4)\n('boy', 4)\n('deck', 4)\n('on', 4)\n('burnind', 4)\n('the', 8)"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}