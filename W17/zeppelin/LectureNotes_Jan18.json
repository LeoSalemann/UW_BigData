{"paragraphs":[{"text":"%pyspark\nlines = sc.textFile(\"hdfs:///tmp/shakesfrom pyspark.sql import SQLContext, Row\n  sqlContext = SQLContext(sc)\n  ##Want to create a DataFrame of People\n  ##Attributes will be Name, Age\n  lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n  parts = lines.map(lambda l: l.split(\",\"))\n  people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n  ##Create the DataFrame\n  peopleDF=sqlContext.createDataFrame(people)peare.txt\")\nratings = lines.map(lambda x: x.split()[2])\n\nresult = ratings.countByValue()","user":"admin","dateUpdated":"2017-01-31T11:27:08+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485904841242_-2134507219","id":"20170131-232041_515463971","result":{"code":"ERROR","type":"TEXT","msg":"Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs:/tmp/shakespeare.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:240)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:240)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:58)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:240)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:934)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:933)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n', JavaObject id=o135), <traceback object at 0x284ce60>)"},"dateCreated":"2017-01-31T11:20:41+0000","dateStarted":"2017-01-31T11:22:43+0000","dateFinished":"2017-01-31T11:22:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9222"},{"text":"%sh\ncat hdfs:///stmp/shakepear.txt\n","user":"admin","dateUpdated":"2017-01-31T11:21:43+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485904871347_-1680088120","id":"20170131-232111_1646076503","result":{"code":"ERROR","type":"TEXT","msg":"cat: hdfs:///stmp/shakepear.txt: No such file or directory\nExitValue: 1"},"dateCreated":"2017-01-31T11:21:11+0000","dateStarted":"2017-01-31T11:21:43+0000","dateFinished":"2017-01-31T11:21:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9223"},{"text":"%pyspark\r\r# from pyspark.sql import SQLContext, Row\r\r  sqlContext = SQLContext(sc)\r  ##Want to create a DataFrame of People\r  ##Attributes will be Name, Age\r  lines = sc.textFile(\"examples/src/main/resources/people.txt\")\r  parts = lines.map(lambda l: l.split(\",\"))\r  people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\r  ##Create the DataFrame\r  peopleDF=sqlContext.createDataFrame(people)","user":"admin","dateUpdated":"2017-01-31T11:27:59+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485904903234_1813694048","id":"20170131-232143_1863390832","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-01-31T11:21:43+0000","dateStarted":"2017-01-31T11:27:59+0000","dateFinished":"2017-01-31T11:27:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9224"},{"text":"%pyspark\r\rdf1 = sc.parallelize( \\\r    [Row(cid='101', name='Alice', age=25, state='ca'), \\\r    Row(cid='102', name='Bob', age=15, state='ny'), \\\r    Row(cid='103', name='Bob', age=23, state='nc'), \\\r    Row(cid='104', name='Ram', age=45, state='fl')]).toDF()\r    \rdf2 = sc.parallelize( \\\r    [Row(cid='101', date='2015-03-12', product='toaster', price=200), \\\r    Row(cid='104', date='2015-04-12', product='iron', price=120), \\\r    Row(cid='102', date='2014-12-31', product='fridge', price=850), \\\r    Row(cid='102', date='2015-02-03', product='cup', price=5)]).toDF()","user":"admin","dateUpdated":"2017-01-31T11:30:43+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485905241936_-987371534","id":"20170131-232721_412558091","result":{"code":"ERROR","type":"TEXT","msg":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6132555238564383205.py\", line 237, in <module>\n    compiledCode = compile(final_code, \"<string>\", \"exec\")\n  File \"<string>\", line 1\n    df1 = sc.parallelize( \\\r    [Row(cid='101', name='Alice', age=25, state='ca'), \\\r    Row(cid='102', name='Bob', age=15, state='ny'), \\\r    Row(cid='103', name='Bob', age=23, state='nc'), \\\r    Row(cid='104', name='Ram', age=45, state='fl')]).toDF()\r    \rdf2 = sc.parallelize( \\\r    [Row(cid='101', date='2015-03-12', product='toaster', price=200), \\\r    Row(cid='104', date='2015-04-12', product='iron', price=120), \\\r    Row(cid='102', date='2014-12-31', product='fridge', price=850), \\\r    Row(cid='102', date='2015-02-03', product='cup', price=5)]).toDF()\n                          \r                                                        \r                                                     \r                                                     \r                                                           \r    \r                       \r                                                                       \r                                                                   \r                                                                     \r                                                                      ^\nSyntaxError: unexpected character after line continuation character\n"},"dateCreated":"2017-01-31T11:27:21+0000","dateStarted":"2017-01-31T11:30:43+0000","dateFinished":"2017-01-31T11:30:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9225"},{"text":"%pyspark\nrdd = sc. parallelize([1,2,3,4])\n# rdd.foreach(lambda x: print(x))\n","user":"admin","dateUpdated":"2017-01-31T11:51:23+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485905299887_-1255264038","id":"20170131-232819_1563045402","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2017-01-31T11:28:19+0000","dateStarted":"2017-01-31T11:51:23+0000","dateFinished":"2017-01-31T11:51:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9226"},{"text":"%spark\nrdd.foreach(lambda x: print x)","user":"admin","dateUpdated":"2017-01-31T11:52:51+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485906540853_-1518298778","id":"20170131-234900_168711385","result":{"code":"ERROR","type":"TEXT","msg":"<console>:1: error: identifier expected but ')' found.\n       rdd.foreach(lambda x: print x)\n                                    ^\n"},"dateCreated":"2017-01-31T11:49:00+0000","dateStarted":"2017-01-31T11:52:51+0000","dateFinished":"2017-01-31T11:52:51+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9227"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1485906678549_1404633159","id":"20170131-235118_1355112786","dateCreated":"2017-01-31T11:51:18+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:9228"}],"name":"LectureNotes_Jan18","id":"2CA8YC1P4","angularObjects":{"2C564P1ZC:shared_process":[],"2C59P4C9R:shared_process":[],"2C3P8E39F:shared_process":[],"2C49HJ26C:shared_process":[],"2C4KWV68J:shared_process":[],"2C3UF1C9F:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}